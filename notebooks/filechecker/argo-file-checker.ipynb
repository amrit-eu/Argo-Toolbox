{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240168e3",
   "metadata": {},
   "source": [
    "## üß≠ Argo FormatChecker Notebook (AMRIT Consortium)\n",
    "\n",
    "This notebook allows you to use the **Argo Format Checker** provided by  **AMRIT** to validate Argo NetCDF files.  \n",
    "The FormatChecker performs both **format** and **content** checks on Argo NetCDF files to ensure compliance with the Argo data standards.\n",
    "\n",
    "üìò **References:**\n",
    "- The Argo NetCDF format is defined in the [Argo User‚Äôs Manual](http://dx.doi.org/10.13155/29825).  \n",
    "- More details and documentation are available on the [Argo Data Management website](https://www.argodatamgt.org/Documentation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687b649",
   "metadata": {},
   "source": [
    "## The main steps to run the checker\n",
    "\n",
    "1. Setup \n",
    "    - before running this notebook, install dependencies with:\n",
    "    ```bash\n",
    "    pip install -r notebooks/requirements.txt\n",
    "    ```\n",
    "2. Please check and fix any Configurations\n",
    "    - Check the API URL and DAC are correct.\n",
    "        - Set the API_BASE_URL to where the File Checker API is running.\n",
    "            - If you are running the File Checker locally inside Docker, set it to the address where the \n",
    "                Docker container is exposing the API `http://localhost:8000`\n",
    "            - If you are running the File Checker in a Kubernetes cluster, use the cluster\n",
    "                URL where the service is exposed. For example:\n",
    "                `https://livkrakentst.clusters.bodc.me/ewetchy/amrit/argo-toolbox/api/file-checker`\n",
    "        - Only one of the URLs should be uncommented based on your environment.\n",
    " \n",
    "2. Run the complete notebook\n",
    "    - This will\n",
    "        - Import necessary packages.\n",
    "        - Sets the config and DAC.\n",
    "        - Checks the connectivity to the API.\n",
    "5. Checking files\n",
    "    - <p>If you need to check few selected files ensure the list of files to check are configured. \n",
    "    - The 'samples' folder has a few files that has been used for demo.\n",
    "6. Optionally run the checker on all files of a deployment.\n",
    "    - <p>If you want to check a whole deployment please check the path is correctly configured and run the corresponding cell.</p>\n",
    "    - The 'profiles' sub directory for a sample float within 'samples' folder has a subset of the full deployment files which is used for demo purpose.\n",
    "7. Results are saved to a csv file if the path is configured.Results are also shown on the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45481bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1Ô∏è‚É£ Importing necessary packages\n",
    "# ===============================\n",
    "\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442c807",
   "metadata": {},
   "source": [
    "## Check the configurations an do any changes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b238d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2Ô∏è‚É£ Configuration\n",
    "# ===============================\n",
    "\n",
    "# Local Docker instance\n",
    "API_BASE_URL=\"http://localhost:8000\"\n",
    "\n",
    "# Kubernetes test instance\n",
    "# API_BASE_URL= \"https://livkrakentst.clusters.bodc.me/ewetchy/amrit/argo-toolbox/api/file-checker\"\n",
    "\n",
    "# Default DAC for validation\n",
    "DEFAULT_DAC = \"bodc\"\n",
    "\n",
    "# Mount location of all the deployment files\n",
    "# if you are running the API locally via Docker, ensure this path matches the volume mount in your Docker setup\n",
    "# for example docker  run --rm --name argo-file-checker2 -p 8000:8000 argo-file-checker\n",
    "\n",
    "# Local path where deployment files are located\n",
    "DEPLOYMENTS_BASE_PATH = \"samples/6902892/profiles\"\n",
    "# files to check along with their patha\n",
    "FILES_TO_CHECK = [\n",
    "    \"samples/6902892/profiles/BR6902892_001.nc\",\n",
    "    \"samples/6902892/profiles/BR6902892_002.nc\"]\n",
    "\n",
    "# Result file for full deployment checks\n",
    "RESULTS=\"results/deployment_files_check_result.csv\"\n",
    "\n",
    "# Endpoints\n",
    "CHECK_FILE_ENDPOINT = \"/check-files\"\n",
    "FULL_DEPLOYMENT_CHECK_ENDPOINT = \"/check-deployment\"\n",
    "\n",
    "# Request settings\n",
    "TIMEOUT = 30  # API request timeout in seconds\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Full URL for file check endpoint\n",
    "FILE_CHECK_URL = f\"{API_BASE_URL}/{CHECK_FILE_ENDPOINT}\"\n",
    "\n",
    "print(f\"üîó API Base URL: {API_BASE_URL}\")\n",
    "print(f\"üèõÔ∏è DAC: {DEFAULT_DAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e8bb9",
   "metadata": {},
   "source": [
    "## Check the API connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "#  3Ô∏è‚É£ Check the API connection\n",
    "# ===============================\n",
    "\n",
    "response = requests.get(f\"{API_BASE_URL}/\", timeout=5)\n",
    "response.raise_for_status()\n",
    "print(\"‚úÖ API is reachable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caba45f",
   "metadata": {},
   "source": [
    "## Check a file or list of files using the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caeee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4Ô∏è‚É£ Checking selected files\n",
    "# ===============================\n",
    "print(f\"Configured files to check: {FILES_TO_CHECK}\")\n",
    "file_paths = []\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    # Save uploaded files to temporary directory\n",
    "    for filepath in FILES_TO_CHECK:\n",
    "        fname = Path(filepath).name  # Get the filename from the path\n",
    "        tmp_path = Path(tmp_dir) / fname\n",
    "\n",
    "        # Copy the file to tmp_dir\n",
    "        with Path(filepath).open(\"rb\") as src:\n",
    "            with tmp_path.open(\"wb\") as dst:\n",
    "                dst.write(src.read())\n",
    "        file_paths.append(tmp_path)\n",
    "\n",
    "    # Prepare files for upload\n",
    "    files_data = []\n",
    "    for file_path in file_paths:\n",
    "        file_obj = Path(file_path).open(\"rb\")\n",
    "        files_data.append((\"files\", (Path(file_path).name, file_obj, \"application/x-netcdf\")))\n",
    "\n",
    "    # Prepare request parameters\n",
    "    params = {\"dac\": DEFAULT_DAC}\n",
    "\n",
    "    # Make the POST request to check files\n",
    "    response = requests.post(\n",
    "            FILE_CHECK_URL,\n",
    "            files=files_data,\n",
    "            params=params,\n",
    "            headers=HEADERS,\n",
    "            timeout=TIMEOUT,\n",
    "    )\n",
    "    if files_data:\n",
    "        # Close all opened file objects\n",
    "        for _, (_, file_obj, _) in files_data:\n",
    "            file_obj.close()\n",
    "\n",
    "        # Process the response\n",
    "        checked_result = response.json()['results']\n",
    "        # Display the results in a DataFrame\n",
    "        if checked_result:\n",
    "            for r in checked_result:\n",
    "                r[\"errors_messages\"] = \"\\n\".join(r.get(\"errors_messages\", []))\n",
    "                r[\"warnings_messages\"] = \"\\n\".join(r.get(\"warnings_messages\", []))\n",
    "            df = pd.DataFrame(checked_result, columns=[\n",
    "                \"file\",\n",
    "                \"result\",\n",
    "                \"phase\",\n",
    "                \"errors_number\",\n",
    "                \"warnings_number\",\n",
    "                \"errors_messages\",\n",
    "                \"warnings_messages\",\n",
    "            ])\n",
    "\n",
    "            # Display the results\n",
    "            with pd.option_context(\n",
    "                'display.max_columns', None,\n",
    "                'display.width', None,\n",
    "                'display.max_colwidth', None\n",
    "                ):\n",
    "                display(df)\n",
    "\n",
    "            # Optional: Save to CSV\n",
    "            if RESULTS:\n",
    "                df.to_csv(RESULTS, index=False, encoding=\"utf-8\")\n",
    "                print(f\"‚úÖ Results saved to:  {RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194dee7",
   "metadata": {},
   "source": [
    "## [Optional] Check all the files of a deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5Ô∏è‚É£ Checking all files in a deployment\n",
    "# ===============================\n",
    "print(\"The deployment folder configured is:\", DEPLOYMENTS_BASE_PATH)\n",
    "deployment_folder = DEPLOYMENTS_BASE_PATH\n",
    "if not Path(deployment_folder).exists():\n",
    "    print(f\"üìÅ Folder does not exist: { deployment_folder}\")\n",
    "else:\n",
    "    # Count number of files in the folder\n",
    "    file_paths = [str(f) for f in Path(deployment_folder).iterdir() if f.is_file()]\n",
    "    num_files = len(file_paths)\n",
    "\n",
    "    if num_files == 0:\n",
    "        print(f\"No files found in deployment folder: {deployment_folder}\" )\n",
    "    else:\n",
    "        print(f\"‚úîÔ∏è Number of files found in deployment folder: {num_files}\" )\n",
    "        files_data = []\n",
    "        # Prepare files for upload\n",
    "        for file_path in file_paths:\n",
    "\n",
    "            if not Path(file_path) or not Path(file_path).is_file():\n",
    "                print(f\"üìÅ Error: Cannot access file: {Path(file_path)}\")\n",
    "                continue\n",
    "            file_obj = Path(file_path).open(\"rb\")\n",
    "            files_data.append((\"files\", (Path(file_path).name, file_obj, \"application/x-netcdf\")))\n",
    "\n",
    "        # set the request parameters\n",
    "        params = {\"dac\": DEFAULT_DAC}\n",
    "\n",
    "        # Send files to API\n",
    "        response = requests.post(\n",
    "            FILE_CHECK_URL,\n",
    "            files=files_data,\n",
    "            params=params,\n",
    "            headers=HEADERS,\n",
    "            timeout=TIMEOUT,\n",
    "        )\n",
    "        if files_data:\n",
    "        # Close all opened file objects\n",
    "            for _, (_, file_obj, _) in files_data:\n",
    "                file_obj.close()\n",
    "\n",
    "        # Process the response\n",
    "        checked_result = response.json()['results']\n",
    "        if checked_result:\n",
    "            for r in checked_result:\n",
    "                r[\"errors_messages\"] = \"\\n\".join(r.get(\"errors_messages\", []))\n",
    "                r[\"warnings_messages\"] = \"\\n\".join(r.get(\"warnings_messages\", []))\n",
    "            df = pd.DataFrame(checked_result, columns=[\n",
    "                \"file\",\n",
    "                \"result\",\n",
    "                \"phase\",\n",
    "                \"errors_number\",\n",
    "                \"warnings_number\",\n",
    "                \"errors_messages\",\n",
    "                \"warnings_messages\",\n",
    "            ])\n",
    "\n",
    "            # Display the results\n",
    "            with pd.option_context(\n",
    "                'display.max_columns', None,\n",
    "                'display.width', None,\n",
    "                'display.max_colwidth', None\n",
    "                ):\n",
    "                display(df)\n",
    "\n",
    "            # Optional: Save to CSV\n",
    "            if RESULTS:\n",
    "                df.to_csv(RESULTS, index=False, encoding=\"utf-8\")\n",
    "                print(f\"‚úÖ Results saved to:  {RESULTS}\")\n",
    "\n",
    "        display(Markdown(\"**‚úÖ Check complete!**\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
