{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240168e3",
   "metadata": {},
   "source": [
    "## üß≠ Argo FormatChecker Notebook (AMRIT Consortium)\n",
    "\n",
    "This pre-configured notebook allows you to use the **Argo Format Checker** provided by  **AMRIT** to validate Argo NetCDF files.  \n",
    "The FormatChecker performs both **format** and **content** checks on Argo NetCDF files to ensure compliance with the Argo data standards.\n",
    "\n",
    "üìò **References:**\n",
    "- The Argo NetCDF format is defined in the [Argo User‚Äôs Manual](http://dx.doi.org/10.13155/29825).  \n",
    "- More details and documentation are available on the [Argo Data Management website](https://www.argodatamgt.org/Documentation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687b649",
   "metadata": {},
   "source": [
    "## The main steps to run the checker\n",
    "1. Assure that you have the file checker api service running. See the main readme of this repository to know how to do it.\n",
    "2. you will need data in the examples/data directory. Two deployment are provided : floats 2903996 (coriolis) and 3901945 (bodc)\n",
    "3. Run the complete notebook\n",
    "    - This will\n",
    "        - Import necessary packages.\n",
    "        - pull data from git LFS if absent\n",
    "        - Sets the config and DAC.\n",
    "        - Checks the connectivity to the API.\n",
    "4. Checking files\n",
    "    - <p>If you need to check few selected files ensure the list of files to check are configured. \n",
    "    - The 'data' folder has a few files that has been used for demo.\n",
    "5. Optionally run the checker on all files of a deployment.\n",
    "    - <p>If you want to check a whole deployment please check the path is correctly configured and run the corresponding cell.</p>\n",
    "6. Results are saved to a csv file if the path is configured.Results are also shown on the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45481bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1Ô∏è‚É£ Importing necessary packages\n",
    "# ===============================\n",
    "\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff82d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 2Ô∏è‚É£  Pull LFS example data if missing\n",
    "# ====================================\n",
    "\n",
    "# check if data and result folders exists :\n",
    "data_dirs = [\n",
    "    Path('../data/2903996'),\n",
    "    Path('../data/3901945'),\n",
    "    Path ('../results')\n",
    "]\n",
    "for data_dir in data_dirs:\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Create directory'{data_dir}'...\")\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#find .nc files and pull it from LFS if not exists:\n",
    "nc_files = []\n",
    "for data_dir in data_dirs:\n",
    "    nc_files.extend(list(data_dir.glob(\"*.nc\")))\n",
    "\n",
    "if not nc_files:\n",
    "    print(\"No files found. Pulling files from LFS..\")\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"lfs\", \"pull\"], check=True, cwd=\"..\")\n",
    "    except:\n",
    "        print(\"Git LFS error. Make sure Git LFS is installed.\")\n",
    "        print(\"   Installation: git lfs install\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442c807",
   "metadata": {},
   "source": [
    "## Check the configurations an do any changes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b238d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3Ô∏è‚É£ Configuration\n",
    "# ===============================\n",
    "\n",
    "# Local Docker instance\n",
    "API_BASE_URL=\"http://host.docker.internal:8080/argo-toolbox/api/file-checker\"\n",
    "\n",
    "# Default DAC for validation\n",
    "DEFAULT_DAC = \"coriolis\" # use \"bodc\" for deployment 3901945\n",
    "\n",
    "# Local path where deployment files are located\n",
    "DEPLOYMENTS_BASE_PATH = \"../data/2903996\" #you can also check files from 3901945\n",
    "\n",
    "# files to check along with their patha\n",
    "FILES_TO_CHECK = [\n",
    "    \"../data/2903996/R2903996_001.nc\",\n",
    "    \"../data/2903996/R2903996_002.nc\"]\n",
    "\n",
    "# Result file for full deployment checks\n",
    "RESULTS=\"../results/deployment_files_check_result.csv\"\n",
    "\n",
    "# Endpoints\n",
    "CHECK_FILE_ENDPOINT = \"/check-files\"\n",
    "\n",
    "# Request settings\n",
    "TIMEOUT = 30  # API request timeout in seconds\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Full URL for file check endpoint\n",
    "FILE_CHECK_URL = f\"{API_BASE_URL}/{CHECK_FILE_ENDPOINT}\"\n",
    "\n",
    "print(f\"üîó API Base URL: {API_BASE_URL}\")\n",
    "print(f\"üèõÔ∏è DAC: {DEFAULT_DAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e8bb9",
   "metadata": {},
   "source": [
    "## Check the API connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4Ô∏è‚É£ Check the API connection\n",
    "# ===============================\n",
    "\n",
    "response = requests.get(f\"{API_BASE_URL}/\", timeout=5)\n",
    "response.raise_for_status()\n",
    "print(\"‚úÖ API is reachable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caba45f",
   "metadata": {},
   "source": [
    "## Check a file or list of files using the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caeee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5Ô∏è‚É£ Checking selected files\n",
    "# ===============================\n",
    "print(f\"Configured files to check: {FILES_TO_CHECK}\")\n",
    "file_paths = []\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    # Save uploaded files to temporary directory\n",
    "    for filepath in FILES_TO_CHECK:\n",
    "        fname = Path(filepath).name  # Get the filename from the path\n",
    "        tmp_path = Path(tmp_dir) / fname\n",
    "\n",
    "        # Copy the file to tmp_dir\n",
    "        with Path(filepath).open(\"rb\") as src:\n",
    "            with tmp_path.open(\"wb\") as dst:\n",
    "                dst.write(src.read())\n",
    "        file_paths.append(tmp_path)\n",
    "\n",
    "    # Prepare files for upload\n",
    "    files_data = []\n",
    "    for file_path in file_paths:\n",
    "        file_obj = Path(file_path).open(\"rb\")\n",
    "        files_data.append((\"files\", (Path(file_path).name, file_obj, \"application/x-netcdf\")))\n",
    "\n",
    "    # Prepare request parameters\n",
    "    params = {\"dac\": DEFAULT_DAC}\n",
    "\n",
    "    # Make the POST request to check files\n",
    "    response = requests.post(\n",
    "            FILE_CHECK_URL,\n",
    "            files=files_data,\n",
    "            params=params,\n",
    "            headers=HEADERS,\n",
    "            timeout=TIMEOUT,\n",
    "    )\n",
    "    if files_data:\n",
    "        # Close all opened file objects\n",
    "        for _, (_, file_obj, _) in files_data:\n",
    "            file_obj.close()\n",
    "\n",
    "        # Process the response\n",
    "        checked_result = response.json()\n",
    "        # Display the results in a DataFrame\n",
    "        if checked_result:\n",
    "            for r in checked_result:\n",
    "                r[\"errors_messages\"] = \"\\n\".join(r.get(\"errors_messages\", []))\n",
    "                r[\"warnings_messages\"] = \"\\n\".join(r.get(\"warnings_messages\", []))\n",
    "            df = pd.DataFrame(checked_result, columns=[\n",
    "                \"file\",\n",
    "                \"result\",\n",
    "                \"phase\",\n",
    "                \"errors_number\",\n",
    "                \"warnings_number\",\n",
    "                \"errors_messages\",\n",
    "                \"warnings_messages\",\n",
    "            ])\n",
    "\n",
    "            # Display the results\n",
    "            with pd.option_context(\n",
    "                'display.max_columns', None,\n",
    "                'display.width', None,\n",
    "                'display.max_colwidth', None\n",
    "                ):\n",
    "                display(df)\n",
    "\n",
    "            # Optional: Save to CSV\n",
    "            if RESULTS:\n",
    "                df.to_csv(RESULTS, index=False, encoding=\"utf-8\")\n",
    "                print(f\"‚úÖ Results saved to:  {RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194dee7",
   "metadata": {},
   "source": [
    "## [Optional] Check all the files of a deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6Ô∏è‚É£ Checking all files in a deployment\n",
    "# ===============================\n",
    "print(\"The deployment folder configured is:\", DEPLOYMENTS_BASE_PATH)\n",
    "deployment_folder = DEPLOYMENTS_BASE_PATH\n",
    "if not Path(deployment_folder).exists():\n",
    "    print(f\"üìÅ Folder does not exist: { deployment_folder}\")\n",
    "else:\n",
    "    # Count number of files in the folder\n",
    "    file_paths = [str(f) for f in Path(deployment_folder).iterdir() if f.is_file()]\n",
    "    num_files = len(file_paths)\n",
    "\n",
    "    if num_files == 0:\n",
    "        print(f\"No files found in deployment folder: {deployment_folder}\" )\n",
    "    else:\n",
    "        print(f\"‚úîÔ∏è Number of files found in deployment folder: {num_files}\" )\n",
    "        files_data = []\n",
    "        # Prepare files for upload\n",
    "        for file_path in file_paths:\n",
    "\n",
    "            if not Path(file_path) or not Path(file_path).is_file():\n",
    "                print(f\"üìÅ Error: Cannot access file: {Path(file_path)}\")\n",
    "                continue\n",
    "            file_obj = Path(file_path).open(\"rb\")\n",
    "            files_data.append((\"files\", (Path(file_path).name, file_obj, \"application/x-netcdf\")))\n",
    "\n",
    "        # set the request parameters\n",
    "        params = {\"dac\": DEFAULT_DAC}\n",
    "\n",
    "        # Send files to API\n",
    "        response = requests.post(\n",
    "            FILE_CHECK_URL,\n",
    "            files=files_data,\n",
    "            params=params,\n",
    "            headers=HEADERS,\n",
    "            timeout=TIMEOUT,\n",
    "        )\n",
    "        if files_data:\n",
    "        # Close all opened file objects\n",
    "            for _, (_, file_obj, _) in files_data:\n",
    "                file_obj.close()\n",
    "\n",
    "        # Process the response\n",
    "        checked_result = response.json()\n",
    "        if checked_result:\n",
    "            for r in checked_result:\n",
    "                r[\"errors_messages\"] = \"\\n\".join(r.get(\"errors_messages\", []))\n",
    "                r[\"warnings_messages\"] = \"\\n\".join(r.get(\"warnings_messages\", []))\n",
    "            df = pd.DataFrame(checked_result, columns=[\n",
    "                \"file\",\n",
    "                \"result\",\n",
    "                \"phase\",\n",
    "                \"errors_number\",\n",
    "                \"warnings_number\",\n",
    "                \"errors_messages\",\n",
    "                \"warnings_messages\",\n",
    "            ])\n",
    "\n",
    "            # Display the results\n",
    "            with pd.option_context(\n",
    "                'display.max_columns', None,\n",
    "                'display.width', None,\n",
    "                'display.max_colwidth', None\n",
    "                ):\n",
    "                display(df)\n",
    "\n",
    "            # Optional: Save to CSV\n",
    "            if RESULTS:\n",
    "                df.to_csv(RESULTS, index=False, encoding=\"utf-8\")\n",
    "                print(f\"‚úÖ Results saved to:  {RESULTS}\")\n",
    "\n",
    "        display(Markdown(\"**‚úÖ Check complete!**\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
